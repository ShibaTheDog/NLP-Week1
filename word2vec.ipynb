{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShibaTheDog/NLP-Week1/blob/branch2/word2vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hX4n9TsbGw-f"
      },
      "source": [
        "##### Copyright 2020 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "0nbI5DtDGw-i"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "haJUNjSB60Kh"
      },
      "source": [
        "# word2vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99d4ky2lWFvn"
      },
      "source": [
        "word2vec is not a singular algorithm, rather, it is a family of model architectures and optimizations that can be used to learn word embeddings from large datasets. Embeddings learned through word2vec have proven to be successful on a variety of downstream natural language processing tasks.\n",
        "\n",
        "> Indented block\n",
        "\n",
        "\n",
        "\n",
        "Note: This tutorial is based on [Efficient estimation of word representations in vector space](https://arxiv.org/pdf/1301.3781.pdf) and [Distributed representations of words and phrases and their compositionality](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf). It is not an exact implementation of the papers. Rather, it is intended to illustrate the key ideas.\n",
        "\n",
        "These papers proposed two methods for learning representations of words:\n",
        "\n",
        "*   **Continuous bag-of-words model**: predicts the middle word based on surrounding context words. The context consists of a few words before and after the current (middle) word. This architecture is called a bag-of-words model as the order of words in the context is not important.\n",
        "*   **Continuous skip-gram model**: predicts words within a certain range before and after the current word in the same sentence. A worked example of this is given below.\n",
        "\n",
        "You'll use the skip-gram approach in this tutorial. First, you'll explore skip-grams and other concepts using a single sentence for illustration. Next, you'll train your own word2vec model on a small dataset. This tutorial also contains code to export the trained embeddings and visualize them in the [TensorFlow Embedding Projector](http://projector.tensorflow.org/).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xP00WlaMWBZC"
      },
      "source": [
        "## Skip-gram and negative sampling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zr2wjv0bW236"
      },
      "source": [
        "While a bag-of-words model predicts a word given the neighboring context, a skip-gram model predicts the context (or neighbors) of a word, given the word itself. The model is trained on skip-grams, which are n-grams that allow tokens to be skipped (see the diagram below for an example). The context of a word can be represented through a set of skip-gram pairs of `(target_word, context_word)` where `context_word` appears in the neighboring context of `target_word`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICjc-McbaVTd"
      },
      "source": [
        "Consider the following sentence of eight words:\n",
        "\n",
        "> The wide road shimmered in the hot sun.\n",
        "\n",
        "The context words for each of the 8 words of this sentence are defined by a window size. The window size determines the span of words on either side of a `target_word` that can be considered a `context word`. Below is a table of skip-grams for target words based on different window sizes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKE87IKT_YT8"
      },
      "source": [
        "Note: For this tutorial, a window size of `n` implies n words on each side with a total window span of 2*n+1 words across a word."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RsCwQ07E8mqU"
      },
      "source": [
        "![word2vec_skipgrams](https://tensorflow.org/text/tutorials/images/word2vec_skipgram.png)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#The idea of this model is that a word that is surrounded by other works will create in a sentence. Therefore, similar words will be able to fit into this model.\n",
        "#The bigger the window, the more words can be captured.\n",
        "\n",
        "#Using this idea, you can gather a bunch of words and what words surround them by training a model to do this."
      ],
      "metadata": {
        "id": "sAM_53xzH5pE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gK1gN1jwkMpU"
      },
      "source": [
        "The training objective of the skip-gram model is to maximize the probability of predicting context words given the target word."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pILO_iAc84e-"
      },
      "source": [
        "![word2vec_skipgram_objective](https://tensorflow.org/text/tutorials/images/word2vec_skipgram_objective.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gsy6TUbtnz_K"
      },
      "source": [
        "where `c` is the size of the training context. The basic skip-gram formulation defines this probability using the softmax function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P81Qavbb9APd"
      },
      "source": [
        "![word2vec_full_softmax](https://tensorflow.org/text/tutorials/images/word2vec_full_softmax.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axZvd-hhotVB"
      },
      "source": [
        "where *v* and *v<sup>'<sup>* are target and context vector representations of words and *W* is vocabulary size."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SoLzxbqSpT6_"
      },
      "source": [
        "Computing the denominator of this formulation involves performing a full softmax over the entire vocabulary words, which are often large (10<sup>5</sup>-10<sup>7</sup>) terms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5VWYtmFzHkU"
      },
      "source": [
        "The [noise contrastive estimation](https://www.tensorflow.org/api_docs/python/tf/nn/nce_loss) (NCE) loss function is an efficient approximation for a full softmax. With an objective to learn word embeddings instead of modeling the word distribution, the NCE loss can be [simplified](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf) to use negative sampling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTZBPf1RsOsg"
      },
      "source": [
        "The simplified negative sampling objective for a target word is to distinguish  the context word from `num_ns` negative samples drawn from noise distribution *P<sub>n</sub>(w)* of words. More precisely, an efficient approximation of full softmax over the vocabulary is, for a skip-gram pair, to pose the loss for a target word as a classification problem between the context word and `num_ns` negative samples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cl0rSfHjt6Mf"
      },
      "source": [
        "A negative sample is defined as a `(target_word, context_word)` pair such that the `context_word` does not appear in the `window_size` neighborhood of the `target_word`. For the example sentence, these are a few potential negative samples (when `window_size` is `2`).\n",
        "\n",
        "```\n",
        "(hot, shimmered)\n",
        "(wide, hot)\n",
        "(wide, sun)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kq0q2uqbucFg"
      },
      "source": [
        "In the next section, you'll generate skip-grams and negative samples for a single sentence. You'll also learn about subsampling techniques and train a classification model for positive and negative training examples later in the tutorial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mk4-Hpe1CH16"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RutaI-Tpev3T"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "import re\n",
        "import string\n",
        "import tqdm\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "10pyUMFkGKVQ"
      },
      "outputs": [],
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XkJ5299Tek6B"
      },
      "outputs": [],
      "source": [
        "SEED = 42\n",
        "AUTOTUNE = tf.data.AUTOTUNE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RW-g5buCHwh3"
      },
      "source": [
        "### Vectorize an example sentence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8TfZIgoQrcP"
      },
      "source": [
        "Consider the following sentence:\n",
        "\n",
        "> The wide road shimmered in the hot sun.\n",
        "\n",
        "Tokenize the sentence:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bsl7jBzV6_KK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bab7e701-4663-4042-f2b7-13f07e40f767"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8\n"
          ]
        }
      ],
      "source": [
        "sentence = \"The wide road shimmered in the hot sun\"\n",
        "tokens = list(sentence.lower().split())\n",
        "print(len(tokens))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "POI7HCg0i6j8",
        "outputId": "d778b0fd-cc57-47ce-fb06-4f97089cde6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['the', 'wide', 'road', 'shimmered', 'in', 'the', 'hot', 'sun']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PU-bs1XtThEw"
      },
      "source": [
        "Create a vocabulary to save mappings from tokens to integer indices:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UdYv1HJUQ8XA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa471dce-b1d9-44cd-94fa-23ef8c2450cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'<pad>': 0, 'the': 1, 'wide': 2, 'road': 3, 'shimmered': 4, 'in': 5, 'hot': 6, 'sun': 7}\n"
          ]
        }
      ],
      "source": [
        "vocab, index = {}, 1  # start indexing from 1\n",
        "vocab['<pad>'] = 0  # add a padding token\n",
        "for token in tokens:\n",
        "  if token not in vocab:\n",
        "    vocab[token] = index\n",
        "    index += 1\n",
        "vocab_size = len(vocab)\n",
        "print(vocab)\n",
        "\n",
        "#You have to tell the model that before the sentence even started, there was nothing. That is what the padding means\n",
        "#If you are using a model with a fixed input size where your sentence is smaller than the expected size, padding is needed ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpuP43Dddasr"
      },
      "source": [
        "Create an inverse vocabulary to save mappings from integer indices to tokens:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o9ULAJYtEvKl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af60e4d1-57da-434b-adf2-d86ade5787fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: '<pad>', 1: 'the', 2: 'wide', 3: 'road', 4: 'shimmered', 5: 'in', 6: 'hot', 7: 'sun'}\n"
          ]
        }
      ],
      "source": [
        "inverse_vocab = {index: token for token, index in vocab.items()}\n",
        "print(inverse_vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3qtuyxIRyii"
      },
      "source": [
        "Vectorize your sentence:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CsB3-9uQQYyl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b94985c-75a8-4432-d832-fcd46b30565f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 2, 3, 4, 5, 1, 6, 7]\n"
          ]
        }
      ],
      "source": [
        "example_sequence = [vocab[word] for word in tokens]\n",
        "print(example_sequence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ox1I28JRIOdM"
      },
      "source": [
        "### Generate skip-grams from one sentence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7NNKAmSiHvy"
      },
      "source": [
        "The `tf.keras.preprocessing.sequence` module provides useful functions that simplify data preparation for word2vec. You can use the `tf.keras.preprocessing.sequence.skipgrams` to generate skip-gram pairs from the `example_sequence` with a given `window_size` from tokens in the range `[0, vocab_size)`.\n",
        "\n",
        "Note: `negative_samples` is set to `0` here, as batching negative samples generated by this function requires a bit of code. You will use another function to perform negative sampling in the next section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "USAJxW4RD7pn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a84914bf-01d5-41f0-c43d-8d9d97918bd9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "26\n"
          ]
        }
      ],
      "source": [
        "window_size = 2\n",
        "#2 refers to the image above, and what the message said. The wide road shimmered... (something like that)\n",
        "\n",
        "positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
        "      example_sequence,\n",
        "      vocabulary_size=vocab_size,\n",
        "      window_size=window_size,\n",
        "      negative_samples=0)\n",
        "print(len(positive_skip_grams))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print((positive_skip_grams))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yvKEhgObkFXv",
        "outputId": "0573b099-6e56-4935-9b27-a3737dead744"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[5, 3], [2, 3], [3, 1], [2, 1], [4, 5], [3, 5], [7, 6], [6, 7], [4, 2], [6, 1], [4, 1], [7, 1], [2, 4], [4, 3], [5, 6], [1, 3], [5, 4], [1, 4], [6, 5], [3, 2], [1, 7], [1, 5], [1, 6], [1, 2], [3, 4], [5, 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uc9uhiMwY-AQ"
      },
      "source": [
        "Print a few positive skip-grams:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SCnqEukIE9pt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c11e9a7-a19e-49cd-b182-acd1dde9c3c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5, 3): (in, road)\n",
            "(2, 3): (wide, road)\n",
            "(3, 1): (road, the)\n",
            "(2, 1): (wide, the)\n",
            "(4, 5): (shimmered, in)\n"
          ]
        }
      ],
      "source": [
        "for target, context in positive_skip_grams[:5]:\n",
        "  print(f\"({target}, {context}): ({inverse_vocab[target]}, {inverse_vocab[context]})\")\n",
        "  #Gather some false cases where the words do not fit.\n",
        "\n",
        "  #The model will later determine what phrases are right and what arent (hopefully)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ua9PkMTISF0"
      },
      "source": [
        "### Negative sampling for one skip-gram"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Esqn8WBfZnEK"
      },
      "source": [
        "The `skipgrams` function returns all positive skip-gram pairs by sliding over a given window span. To produce additional skip-gram pairs that would serve as negative samples for training, you need to sample random words from the vocabulary. Use the `tf.random.log_uniform_candidate_sampler` function to sample `num_ns` number of negative samples for a given target word in a window. You can call the function on one skip-grams's target word and pass the context word as true class to exclude it from being sampled.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgH3aSvw3xTD"
      },
      "source": [
        "Key point: `num_ns` (the number of negative samples per a positive context word) in the `[5, 20]` range is [shown to work](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf) best for smaller datasets, while `num_ns` in the `[2, 5]` range suffices for larger datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m_LmdzqIGr5L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3611da15-9b58-4466-d228-487a34f48e13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([6 4 2 0], shape=(4,), dtype=int64)\n",
            "['hot', 'shimmered', 'wide', '<pad>']\n"
          ]
        }
      ],
      "source": [
        "# Get target and context words for one positive skip-gram.\n",
        "target_word, context_word = positive_skip_grams[0]\n",
        "\n",
        "# Set the number of negative samples per positive context.\n",
        "num_ns = 4\n",
        "\n",
        "context_class = tf.reshape(tf.constant(context_word, dtype=\"int64\"), (1, 1))\n",
        "negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
        "    true_classes=context_class,  # class that should be sampled as 'positive'\n",
        "    num_true=1,  # each positive skip-gram has 1 positive context class\n",
        "    num_sampled=num_ns,  # number of negative context words to sample\n",
        "    unique=True,  # all the negative samples should be unique\n",
        "    range_max=vocab_size,  # pick index of the samples from [0, vocab_size]\n",
        "    seed=SEED,  # seed for reproducibility\n",
        "    name=\"negative_sampling\"  # name of this operation\n",
        ")\n",
        "print(negative_sampling_candidates)\n",
        "print([inverse_vocab[index.numpy()] for index in negative_sampling_candidates])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SEED"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qlKuKLvNsiE-",
        "outputId": "d37c50f9-5d9e-4f3d-ca1e-af76d660668b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "42"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "target_word,context_word"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NcqAu4zGluBD",
        "outputId": "0d21276a-b0f7-4d4f-e63f-3d13147d7d45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8MSxWCrLIalp"
      },
      "source": [
        "### Construct one training example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6uEWdj8vKKv"
      },
      "source": [
        "For a given positive `(target_word, context_word)` skip-gram, you now also have `num_ns` negative sampled context words that do not appear in the window size neighborhood of `target_word`. Batch the `1` positive `context_word` and `num_ns` negative context words into one tensor. This produces a set of positive skip-grams (labeled as `1`) and negative samples (labeled as `0`) for each target word."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zSiZwifuLvHf"
      },
      "outputs": [],
      "source": [
        "# Reduce a dimension so you can use concatenation (in the next step).\n",
        "squeezed_context_class = tf.squeeze(context_class, 1)\n",
        "\n",
        "# Concatenate a positive context word with negative sampled words.\n",
        "context = tf.concat([squeezed_context_class, negative_sampling_candidates], 0)\n",
        "\n",
        "# Label the first context word as `1` (positive) followed by `num_ns` `0`s (negative).\n",
        "label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n",
        "target = target_word\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIJeoFCAwtXJ"
      },
      "source": [
        "Check out the context and the corresponding labels for the target word from the skip-gram example above:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tzyCPCuZwmdL"
      },
      "outputs": [],
      "source": [
        "print(f\"target_index    : {target}\")\n",
        "print(f\"target_word     : {inverse_vocab[target_word]}\")\n",
        "print(f\"context_indices : {context}\")\n",
        "print(f\"context_words   : {[inverse_vocab[c.numpy()] for c in context]}\")\n",
        "print(f\"label           : {label}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBtTcUVQr8EO"
      },
      "source": [
        "A tuple of `(target, context, label)` tensors constitutes one training example for training your skip-gram negative sampling word2vec model. Notice that the target is of shape `(1,)` while the context and label are of shape `(1+num_ns,)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x-FwkR8jx9-Z"
      },
      "outputs": [],
      "source": [
        "print(\"target  :\", target)\n",
        "print(\"context :\", context)\n",
        "print(\"label   :\", label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bRJIlow4Dlv"
      },
      "source": [
        "### Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWkuha0oykG5"
      },
      "source": [
        "This diagram summarizes the procedure of generating a training example from a sentence:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KlwdiAa9crJ"
      },
      "source": [
        "![word2vec_negative_sampling](https://tensorflow.org/text/tutorials/images/word2vec_negative_sampling.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37e53f07f67c"
      },
      "source": [
        "Notice that the words `temperature` and `code` are not part of the input sentence. They belong to the vocabulary like certain other indices used in the diagram above."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To get the full notebook, that includes training on a dataset, please see the full tutorial below."
      ],
      "metadata": {
        "id": "Khu2cqthEfFU"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOpGoE2T-YXS"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/text/tutorials/word2vec\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/text/blob/master/docs/tutorials/word2vec.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/text/blob/master/docs/tutorials/word2vec.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View on GitHub</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://storage.googleapis.com/tensorflow_docs/text/docs/tutorials/word2vec.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
        "  </td>\n",
        "</table>"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}